{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7050adf",
   "metadata": {},
   "source": [
    "# 1. Overview & Objectives\n",
    "\n",
    "This notebook implements and evaluates a broad range of **classical machine learning regression models**\n",
    "for **univariate daily weather forecasting** using Meteostat data.\n",
    "\n",
    "The focus of this notebook is:\n",
    "- Tree-based models\n",
    "- Linear and robust regression models\n",
    "- Distance-based regressors\n",
    "- Kernel-based methods\n",
    "- Neural network regressors (non-sequential)\n",
    "\n",
    "All models are trained using a **time-aware feature engineering approach**\n",
    "based on lagged values of the target variable (`tavg`).\n",
    "\n",
    "### Implemented model families\n",
    "- Linear models: Linear, Ridge, Huber, Tweedie\n",
    "- Distance-based: KNN, Radius Neighbors\n",
    "- Tree-based: Decision Tree, Random Forest, HistGB\n",
    "- Boosting: LightGBM, XGBoost, XGBoost Random Forest\n",
    "- Neural: MLP Regressor\n",
    "- Kernel: Support Vector Regression\n",
    "\n",
    "### Outputs\n",
    "- CSV files with evaluation metrics for validation & test splits\n",
    "- Stored best model configurations for later visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5596316",
   "metadata": {},
   "source": [
    "# 2. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bc4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the helper notebooks\n",
    "\n",
    "## Enable imports from .ipynb files\n",
    "import import_ipynb  \n",
    "import sys\n",
    "sys.path.append(\"code\")\n",
    "\n",
    "## Importing the helper notebooks as modules\n",
    "from splitting import split_time_series\n",
    "from metrics import evaluate_and_save, load_best_models\n",
    "\n",
    "# Notebook specific imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "from mlforecast import MLForecast\n",
    "from mlforecast.target_transforms import Differences\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge, HuberRegressor, TweedieRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab7415",
   "metadata": {},
   "source": [
    "# 3. Load Data & Train/Val/Test Split\n",
    "use `split_time_series()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe18b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = split_time_series()\n",
    "\n",
    "train_df = splits[\"train\"]\n",
    "val_df   = splits[\"val\"]\n",
    "test_df  = splits[\"test\"]\n",
    "\n",
    "TARGET_COL = \"tavg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d665e34",
   "metadata": {},
   "source": [
    "# 4. Model Definition\n",
    "\n",
    "#### Model families and names\n",
    "- **Linear models**: LinearRegression, HuberRegression, RidgeRegression, TweedieRegression\n",
    "- **Tree / Ensemble models**:\n",
    "  - RandomForest (`RandomForest_ne{n_estimators}_md{max_depth}`)\n",
    "  - LightGBM (`LightGBM_nl{num_leaves}_lr{learning_rate}_ne{n_estimators}`)\n",
    "  - XGBoost (`XGBoost_md{max_depth}_lr{learning_rate}_ne{n_estimators}`)\n",
    "  - XGBRF, DecisionTree, HistGradientBoosting\n",
    "- **Kernel / distance-based models**:\n",
    "  - SVR (`SVR_C{C}_g{gamma}_e{epsilon}`)\n",
    "  - KNN (`KNN_k{n_neighbors}_w{weights}`)\n",
    "  - RadiusNeighbors\n",
    "- **Neural network**:\n",
    "- MLP (`MLP_h{hidden_units}_mi{max_iter}`)\n",
    "\n",
    "#### Hyperparameters (searched values)\n",
    "- **RandomForest**: `n_estimators ∈ {50,100,200,300,400,500}`, `max_depth ∈ {5,10,20,30}`\n",
    "- **LightGBM**: `num_leaves ∈ {31,50,73}`, `learning_rate ∈ {0.05,0.1,0.15}`, `n_estimators ∈ {50,100,200,300}`\n",
    "- **XGBoost**: `max_depth ∈ {3,5,9}`, `learning_rate ∈ {0.05,0.1}`, `n_estimators ∈ {100,200}`\n",
    "- **SVR**: `C ∈ {1,10,20,30}`, `gamma ∈ {scale, 0.1}`, `epsilon ∈ {0.1,0.2,0.3}`\n",
    "- **KNN**: `n_neighbors ∈ {3,4,5,6,10}`, `weights ∈ {uniform,distance}`\n",
    "- **MLP**: `hidden_layer_sizes ∈ {(10),(20),(50),(100)}`, `max_iter ∈ {500,1000,1500}`\n",
    "- **Fixed settings**: `random_state = 42` where applicable\n",
    "\n",
    "**Why `mlforecast`?**\n",
    "Most sklearn regressors are **time-agnostic**. `mlforecast` transforms time series into **supervised learning datasets using lag features**, enabling standard regressors to model temporal dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "216805e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PARAM_REGISTRY = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ae8d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"HuberRegression\": HuberRegressor(),\n",
    "    \"RidgeRegression\": Ridge(),\n",
    "    \"TweedieRegression\": TweedieRegressor(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ae57cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in lin_models.items():\n",
    "    MODEL_PARAM_REGISTRY[name] = {\n",
    "        \"model_class\": model.__class__,\n",
    "        \"params\": model.get_params()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e66a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_models = {}\n",
    "\n",
    "for n_estimators in [50, 100, 200, 300, 400, 500]:\n",
    "    for max_depth in [5, 10, 20, 30]:\n",
    "        name = f\"RandomForest_ne{n_estimators}_md{max_depth}\"\n",
    "\n",
    "        rf_models[name] = RandomForestRegressor(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        MODEL_PARAM_REGISTRY[name] = {\n",
    "            \"model_class\": RandomForestRegressor,\n",
    "            \"params\": {\n",
    "                \"n_estimators\": n_estimators,\n",
    "                \"max_depth\": max_depth,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92e09974",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_models = {}\n",
    "\n",
    "for num_leaves in [31, 50, 73]:\n",
    "    for lr in [0.05, 0.1, 0.15]:\n",
    "        for n_estimators in [50, 100, 200, 300]:\n",
    "            name = f\"LightGBM_nl{num_leaves}_lr{lr}_ne{n_estimators}\"\n",
    "\n",
    "            lgbm_models[name] = LGBMRegressor(\n",
    "                num_leaves=num_leaves,\n",
    "                learning_rate=lr,\n",
    "                n_estimators=n_estimators,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            MODEL_PARAM_REGISTRY[name] = {\n",
    "                \"model_class\": LGBMRegressor,\n",
    "                \"params\": {\n",
    "                    \"num_leaves\": num_leaves,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"n_estimators\": n_estimators,\n",
    "                    \"random_state\": 42\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb8f1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_models = {}\n",
    "\n",
    "for max_depth in [3, 5, 9]:\n",
    "    for lr in [0.05, 0.1]:\n",
    "        for n_estimators in [100, 200]:\n",
    "            name = f\"XGBoost_md{max_depth}_lr{lr}_ne{n_estimators}\"\n",
    "\n",
    "            xgb_models[name] = XGBRegressor(\n",
    "                max_depth=max_depth,\n",
    "                learning_rate=lr,\n",
    "                n_estimators=n_estimators,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            MODEL_PARAM_REGISTRY[name] = {\n",
    "                \"model_class\": XGBRegressor,\n",
    "                \"params\": {\n",
    "                    \"max_depth\": max_depth,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"n_estimators\": n_estimators,\n",
    "                    \"random_state\": 42\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f82795",
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_models = {}\n",
    "\n",
    "for C in [1, 10, 20, 30]:\n",
    "    for gamma in [\"scale\", 0.1]:\n",
    "        for epsilon in [0.1, 0.2, 0.3]:\n",
    "            name = f\"SVR_C{C}_g{gamma}_e{epsilon}\"\n",
    "\n",
    "            svr_models[name] = svm.SVR(\n",
    "                C=C,\n",
    "                gamma=gamma,\n",
    "                epsilon=epsilon\n",
    "            )\n",
    "\n",
    "            MODEL_PARAM_REGISTRY[name] = {\n",
    "                \"model_class\": svm.SVR,\n",
    "                \"params\": {\n",
    "                    \"C\": C,\n",
    "                    \"gamma\": gamma,\n",
    "                    \"epsilon\": epsilon\n",
    "                }\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fcf6f863",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_models = {}\n",
    "\n",
    "for n_neighbors in [3, 4, 5, 6, 10]:\n",
    "    for weights in [\"uniform\", \"distance\"]:\n",
    "        name = f\"KNN_k{n_neighbors}_w{weights}\"\n",
    "\n",
    "        knn_models[name] = KNeighborsRegressor(\n",
    "            n_neighbors=n_neighbors,\n",
    "            weights=weights\n",
    "        )\n",
    "\n",
    "        MODEL_PARAM_REGISTRY[name] = {\n",
    "            \"model_class\": KNeighborsRegressor,\n",
    "            \"params\": {\n",
    "                \"n_neighbors\": n_neighbors,\n",
    "                \"weights\": weights\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e67bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_models = {}\n",
    "\n",
    "for hls in [(10,), (20,), (50,), (100,)]:\n",
    "    for max_iter in [500, 1000, 1500]:\n",
    "        name = f\"MLP_h{hls[0]}_mi{max_iter}\"\n",
    "\n",
    "        mlp_models[name] = MLPRegressor(\n",
    "            hidden_layer_sizes=hls,\n",
    "            max_iter=max_iter,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        MODEL_PARAM_REGISTRY[name] = {\n",
    "            \"model_class\": MLPRegressor,\n",
    "            \"params\": {\n",
    "                \"hidden_layer_sizes\": hls,\n",
    "                \"max_iter\": max_iter,\n",
    "                \"random_state\": 42\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa0b9549",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_models = {\n",
    "    \"RadiusNeighbors\": RadiusNeighborsRegressor(radius=1e5),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(criterion=\"squared_error\"),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingRegressor(loss=\"absolute_error\"),\n",
    "    \"XGBRF\": XGBRFRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "tree_models.update(rf_models)\n",
    "tree_models.update(lgbm_models)\n",
    "tree_models.update(xgb_models)\n",
    "tree_models.update(svr_models)\n",
    "tree_models.update(knn_models)\n",
    "tree_models.update(mlp_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557587d",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ff051d",
   "metadata": {},
   "source": [
    "#### Feature Scaling for distance/kernel models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b312ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scale_models = [\"SVR\", \"KNN\", \"RadiusNeighbors\", \"MLP\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e61af1",
   "metadata": {},
   "source": [
    "# 5. Training  \n",
    "For each model:  \n",
    "- Fit on training data  \n",
    "- Lag structure:\n",
    "  - short-term memory: `t-1` to `t-29`\n",
    "  - seasonal memory: `t-60`, `t-91`, `t-182`, `t-365`\n",
    "  - captures: daily autocorrelation, weekly/monthly effects, annual seasonality\n",
    "\n",
    "**Additional lag transforms** (linear models only):\n",
    "- Rolling statistics:\n",
    "  - 7-day mean and std\n",
    "  - 30-day mean\n",
    "- Purpose: \n",
    "  - encode local trends and volatility\n",
    "  - improve linear model expressiveness\n",
    "\n",
    "**Stationarity Handling**\n",
    "- Differencing `(Δt = 1)` applied only to linear models\n",
    "- Reason:\n",
    "  - linear regression assumes stable mean\n",
    "  - weather data is non-stationary\n",
    "- tree-based models handle the non-stationarity implicitly -> thus no differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d78bf003",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lags\n",
    "LAGS = list(range(1, 30)) + [60, 91, 182, 365]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "841fd58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlforecast.lag_transforms import RollingMean, RollingStd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583fb08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLForecast setup\n",
    "lin_fcst = MLForecast(\n",
    "    models=lin_models,\n",
    "    freq=\"D\",\n",
    "    lags=LAGS,\n",
    "    lag_transforms={\n",
    "        7: [RollingMean(7), RollingStd(7)],\n",
    "        30: [RollingMean(30)],\n",
    "    },\n",
    "    target_transforms=[Differences([1])]\n",
    ")\n",
    "\n",
    "tree_fcst = MLForecast(\n",
    "    models=tree_models,\n",
    "    freq=\"D\",\n",
    "    lags=LAGS,\n",
    "    target_transforms=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6948e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporarily add a column because MLForecast requires a unique_id column for panel data\n",
    "train_temp = train_df.copy()\n",
    "train_temp[\"unique_id\"] = \"station_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07e67c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002387 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001068 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001117 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001891 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001858 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002386 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002292 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003264 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002587 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003338 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002644 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003565 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003320 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001509 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003364 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003156 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003458 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003429 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003240 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001651 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003554 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003352 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003412 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002802 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003381 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003426 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003249 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8415\n",
      "[LightGBM] [Info] Number of data points in the train set: 12784, number of used features: 33\n",
      "[LightGBM] [Info] Start training from score 6.950070\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLForecast(models=[RadiusNeighbors, DecisionTree, HistGradientBoosting, XGBRF, RandomForest_ne50_md5, RandomForest_ne50_md10, RandomForest_ne50_md20, RandomForest_ne50_md30, RandomForest_ne100_md5, RandomForest_ne100_md10, RandomForest_ne100_md20, RandomForest_ne100_md30, RandomForest_ne200_md5, RandomForest_ne200_md10, RandomForest_ne200_md20, RandomForest_ne200_md30, RandomForest_ne300_md5, RandomForest_ne300_md10, RandomForest_ne300_md20, RandomForest_ne300_md30, RandomForest_ne400_md5, RandomForest_ne400_md10, RandomForest_ne400_md20, RandomForest_ne400_md30, RandomForest_ne500_md5, RandomForest_ne500_md10, RandomForest_ne500_md20, RandomForest_ne500_md30, LightGBM_nl31_lr0.05_ne50, LightGBM_nl31_lr0.05_ne100, LightGBM_nl31_lr0.05_ne200, LightGBM_nl31_lr0.05_ne300, LightGBM_nl31_lr0.1_ne50, LightGBM_nl31_lr0.1_ne100, LightGBM_nl31_lr0.1_ne200, LightGBM_nl31_lr0.1_ne300, LightGBM_nl31_lr0.15_ne50, LightGBM_nl31_lr0.15_ne100, LightGBM_nl31_lr0.15_ne200, LightGBM_nl31_lr0.15_ne300, LightGBM_nl50_lr0.05_ne50, LightGBM_nl50_lr0.05_ne100, LightGBM_nl50_lr0.05_ne200, LightGBM_nl50_lr0.05_ne300, LightGBM_nl50_lr0.1_ne50, LightGBM_nl50_lr0.1_ne100, LightGBM_nl50_lr0.1_ne200, LightGBM_nl50_lr0.1_ne300, LightGBM_nl50_lr0.15_ne50, LightGBM_nl50_lr0.15_ne100, LightGBM_nl50_lr0.15_ne200, LightGBM_nl50_lr0.15_ne300, LightGBM_nl73_lr0.05_ne50, LightGBM_nl73_lr0.05_ne100, LightGBM_nl73_lr0.05_ne200, LightGBM_nl73_lr0.05_ne300, LightGBM_nl73_lr0.1_ne50, LightGBM_nl73_lr0.1_ne100, LightGBM_nl73_lr0.1_ne200, LightGBM_nl73_lr0.1_ne300, LightGBM_nl73_lr0.15_ne50, LightGBM_nl73_lr0.15_ne100, LightGBM_nl73_lr0.15_ne200, LightGBM_nl73_lr0.15_ne300, XGBoost_md3_lr0.05_ne100, XGBoost_md3_lr0.05_ne200, XGBoost_md3_lr0.1_ne100, XGBoost_md3_lr0.1_ne200, XGBoost_md5_lr0.05_ne100, XGBoost_md5_lr0.05_ne200, XGBoost_md5_lr0.1_ne100, XGBoost_md5_lr0.1_ne200, XGBoost_md9_lr0.05_ne100, XGBoost_md9_lr0.05_ne200, XGBoost_md9_lr0.1_ne100, XGBoost_md9_lr0.1_ne200, SVR_C1_gscale_e0.1, SVR_C1_gscale_e0.2, SVR_C1_gscale_e0.3, SVR_C1_g0.1_e0.1, SVR_C1_g0.1_e0.2, SVR_C1_g0.1_e0.3, SVR_C10_gscale_e0.1, SVR_C10_gscale_e0.2, SVR_C10_gscale_e0.3, SVR_C10_g0.1_e0.1, SVR_C10_g0.1_e0.2, SVR_C10_g0.1_e0.3, SVR_C20_gscale_e0.1, SVR_C20_gscale_e0.2, SVR_C20_gscale_e0.3, SVR_C20_g0.1_e0.1, SVR_C20_g0.1_e0.2, SVR_C20_g0.1_e0.3, SVR_C30_gscale_e0.1, SVR_C30_gscale_e0.2, SVR_C30_gscale_e0.3, SVR_C30_g0.1_e0.1, SVR_C30_g0.1_e0.2, SVR_C30_g0.1_e0.3, KNN_k3_wuniform, KNN_k3_wdistance, KNN_k4_wuniform, KNN_k4_wdistance, KNN_k5_wuniform, KNN_k5_wdistance, KNN_k6_wuniform, KNN_k6_wdistance, KNN_k10_wuniform, KNN_k10_wdistance, MLP_h10_mi500, MLP_h10_mi1000, MLP_h10_mi1500, MLP_h20_mi500, MLP_h20_mi1000, MLP_h20_mi1500, MLP_h50_mi500, MLP_h50_mi1000, MLP_h50_mi1500, MLP_h100_mi500, MLP_h100_mi1000, MLP_h100_mi1500], freq=D, lag_features=['lag1', 'lag2', 'lag3', 'lag4', 'lag5', 'lag6', 'lag7', 'lag8', 'lag9', 'lag10', 'lag11', 'lag12', 'lag13', 'lag14', 'lag15', 'lag16', 'lag17', 'lag18', 'lag19', 'lag20', 'lag21', 'lag22', 'lag23', 'lag24', 'lag25', 'lag26', 'lag27', 'lag28', 'lag29', 'lag60', 'lag91', 'lag182', 'lag365'], date_features=[], num_threads=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit models\n",
    "lin_fcst.fit(\n",
    "    train_temp,\n",
    "    id_col=\"unique_id\",\n",
    "    time_col=\"time\",\n",
    "    target_col=TARGET_COL,\n",
    ")\n",
    "\n",
    "tree_fcst.fit(\n",
    "    train_temp,\n",
    "    id_col=\"unique_id\",\n",
    "    time_col=\"time\",\n",
    "    target_col=TARGET_COL,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a59adc",
   "metadata": {},
   "source": [
    "# 6. Forecasting  \n",
    "- Produce forecasts for validation and test horizons\n",
    "- Forecast Horizon = full validation / test length\n",
    "- Forecasts produced:  \n",
    "  - Autoregressively\n",
    "  - Using model predictions as future lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8915eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n",
      "c:\\Public\\TIS3IL-WS25-Project\\.venv\\Lib\\site-packages\\utilsforecast\\processing.py:124: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[names] = values\n"
     ]
    }
   ],
   "source": [
    "#Forecasting\n",
    "val_temp = val_df.copy()\n",
    "val_temp[\"unique_id\"] = \"station_1\" #MLForecast requires id_col for panel data, even for a single series\n",
    "H_VAL = len(val_temp)\n",
    "val_preds_lin = lin_fcst.predict(H_VAL)\n",
    "val_preds_tree = tree_fcst.predict(H_VAL)\n",
    "\n",
    "test_temp = test_df.copy()\n",
    "test_temp[\"unique_id\"] = \"station_1\"\n",
    "H_TEST = len(test_temp)\n",
    "test_preds_lin = lin_fcst.predict(H_TEST)\n",
    "test_preds_tree = tree_fcst.predict(H_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5702e4f9",
   "metadata": {},
   "source": [
    "# 7. Evaluation (Using Shared Metrics Function)  \n",
    "- Applied `evaluate_and_save()` to each model  \n",
    "- Results saved as CSV into `data/models/`  \n",
    "- Display sorted results table  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed3ec582",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_FILE = \"../data/models/ml_models_results.csv\"\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "064c7d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_split(y_true, forecasts, split_name):\n",
    "    forecasts = forecasts.drop(columns=[c for c in [\"unique_id\"] if c in forecasts.columns])\n",
    "    for model_name in forecasts.columns:\n",
    "        metrics_dict = evaluate_and_save(\n",
    "            y_true=y_true,\n",
    "            y_pred=forecasts[model_name].values,\n",
    "            model_name=model_name,\n",
    "            impl_name=\"ml\",\n",
    "            split_name=split_name,\n",
    "            out_filename=\"ml_models_results.csv\" #temporary CSV to avoid overwriting\n",
    "        )\n",
    "        results.append(metrics_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate validation and test\n",
    "evaluate_split(val_df[TARGET_COL].values, val_preds_lin, \"val\")\n",
    "evaluate_split(val_df[TARGET_COL].values, val_preds_tree, \"val\")\n",
    "evaluate_split(test_df[TARGET_COL].values, test_preds_lin, \"test\")\n",
    "evaluate_split(test_df[TARGET_COL].values, test_preds_tree, \"test\")\n",
    "\n",
    "#Combine results and keep top 3 per split\n",
    "results_df = pd.DataFrame(results)\n",
    "top_val = results_df[results_df[\"Split\"]==\"val\"].sort_values(\"MAE\").head(3)\n",
    "top_test = results_df[results_df[\"Split\"]==\"test\"].sort_values(\"MAE\").head(3)\n",
    "best_models = pd.concat([top_val, top_test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df909443",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model_names = best_models[\"Model\"].unique()\n",
    "\n",
    "BEST_MODEL_CONFIGS = {\n",
    "    name: MODEL_PARAM_REGISTRY[name]\n",
    "    for name in top_model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17b0e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save top models to CSV\n",
    "best_models.to_csv(OUT_FILE, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91e307ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All model metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Impl</th>\n",
       "      <th>Split</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>OPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>MLP_h20_mi500</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>3.984510</td>\n",
       "      <td>5.030306</td>\n",
       "      <td>6.453597e+09</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.759585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>MLP_h20_mi1000</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>3.984510</td>\n",
       "      <td>5.030306</td>\n",
       "      <td>6.453597e+09</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.759585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>MLP_h20_mi1500</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>3.984510</td>\n",
       "      <td>5.030306</td>\n",
       "      <td>6.453597e+09</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.759585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>KNN_k4_wdistance</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>4.224437</td>\n",
       "      <td>5.430456</td>\n",
       "      <td>3.576457e+09</td>\n",
       "      <td>0.052938</td>\n",
       "      <td>0.719814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>KNN_k4_wuniform</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>4.369636</td>\n",
       "      <td>5.511018</td>\n",
       "      <td>9.654018e+09</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.711439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>KNN_k5_wdistance</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>4.458703</td>\n",
       "      <td>5.708083</td>\n",
       "      <td>3.534339e+09</td>\n",
       "      <td>0.032122</td>\n",
       "      <td>0.690433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>KNN_k6_wuniform</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>4.491402</td>\n",
       "      <td>5.673430</td>\n",
       "      <td>5.468750e+09</td>\n",
       "      <td>0.069097</td>\n",
       "      <td>0.694181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>KNN_k3_wdistance</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>4.778261</td>\n",
       "      <td>6.168883</td>\n",
       "      <td>3.399476e+09</td>\n",
       "      <td>0.006476</td>\n",
       "      <td>0.638435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>LightGBM_nl73_lr0.15_ne300</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>5.183773</td>\n",
       "      <td>6.787466</td>\n",
       "      <td>2.784946e+09</td>\n",
       "      <td>0.120175</td>\n",
       "      <td>0.562288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>MLP_h100_mi500</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>5.222226</td>\n",
       "      <td>6.750290</td>\n",
       "      <td>3.831620e+09</td>\n",
       "      <td>0.120363</td>\n",
       "      <td>0.567069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Model Impl Split       MAE      RMSE          MAPE  \\\n",
       "247               MLP_h20_mi500   ml  test  3.984510  5.030306  6.453597e+09   \n",
       "248              MLP_h20_mi1000   ml  test  3.984510  5.030306  6.453597e+09   \n",
       "249              MLP_h20_mi1500   ml  test  3.984510  5.030306  6.453597e+09   \n",
       "237            KNN_k4_wdistance   ml  test  4.224437  5.430456  3.576457e+09   \n",
       "236             KNN_k4_wuniform   ml  test  4.369636  5.511018  9.654018e+09   \n",
       "239            KNN_k5_wdistance   ml  test  4.458703  5.708083  3.534339e+09   \n",
       "240             KNN_k6_wuniform   ml  test  4.491402  5.673430  5.468750e+09   \n",
       "235            KNN_k3_wdistance   ml  test  4.778261  6.168883  3.399476e+09   \n",
       "197  LightGBM_nl73_lr0.15_ne300   ml  test  5.183773  6.787466  2.784946e+09   \n",
       "253              MLP_h100_mi500   ml  test  5.222226  6.750290  3.831620e+09   \n",
       "\n",
       "          OPE        R2  \n",
       "247  0.074321  0.759585  \n",
       "248  0.074321  0.759585  \n",
       "249  0.074321  0.759585  \n",
       "237  0.052938  0.719814  \n",
       "236  0.000145  0.711439  \n",
       "239  0.032122  0.690433  \n",
       "240  0.069097  0.694181  \n",
       "235  0.006476  0.638435  \n",
       "197  0.120175  0.562288  \n",
       "253  0.120363  0.567069  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display results\n",
    "print(\"All model metrics:\")\n",
    "#display(results_df.sort_values([\"Split\", \"MAE\"]))\n",
    "results_df.sort_values([\"Split\", \"MAE\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10d20d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 models per split saved to CSV:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Impl</th>\n",
       "      <th>Split</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>OPE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP_h20_mi500</td>\n",
       "      <td>ml</td>\n",
       "      <td>val</td>\n",
       "      <td>4.270973</td>\n",
       "      <td>5.432609</td>\n",
       "      <td>1.042164e+09</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.740650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP_h20_mi1000</td>\n",
       "      <td>ml</td>\n",
       "      <td>val</td>\n",
       "      <td>4.270973</td>\n",
       "      <td>5.432609</td>\n",
       "      <td>1.042164e+09</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.740650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP_h20_mi1500</td>\n",
       "      <td>ml</td>\n",
       "      <td>val</td>\n",
       "      <td>4.270973</td>\n",
       "      <td>5.432609</td>\n",
       "      <td>1.042164e+09</td>\n",
       "      <td>0.003407</td>\n",
       "      <td>0.740650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP_h20_mi500</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>3.984510</td>\n",
       "      <td>5.030306</td>\n",
       "      <td>6.453597e+09</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.759585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP_h20_mi1000</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>3.984510</td>\n",
       "      <td>5.030306</td>\n",
       "      <td>6.453597e+09</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.759585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP_h20_mi1500</td>\n",
       "      <td>ml</td>\n",
       "      <td>test</td>\n",
       "      <td>3.984510</td>\n",
       "      <td>5.030306</td>\n",
       "      <td>6.453597e+09</td>\n",
       "      <td>0.074321</td>\n",
       "      <td>0.759585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model Impl Split       MAE      RMSE          MAPE       OPE  \\\n",
       "0   MLP_h20_mi500   ml   val  4.270973  5.432609  1.042164e+09  0.003407   \n",
       "1  MLP_h20_mi1000   ml   val  4.270973  5.432609  1.042164e+09  0.003407   \n",
       "2  MLP_h20_mi1500   ml   val  4.270973  5.432609  1.042164e+09  0.003407   \n",
       "3   MLP_h20_mi500   ml  test  3.984510  5.030306  6.453597e+09  0.074321   \n",
       "4  MLP_h20_mi1000   ml  test  3.984510  5.030306  6.453597e+09  0.074321   \n",
       "5  MLP_h20_mi1500   ml  test  3.984510  5.030306  6.453597e+09  0.074321   \n",
       "\n",
       "         R2  \n",
       "0  0.740650  \n",
       "1  0.740650  \n",
       "2  0.740650  \n",
       "3  0.759585  \n",
       "4  0.759585  \n",
       "5  0.759585  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Top 3 models per split saved to CSV:\")\n",
    "display(best_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b029ab",
   "metadata": {},
   "source": [
    "# 8. Conclusions  \n",
    "#### Short wrap-up:  \n",
    "- Which model family performed best here?  \n",
    "- Any issues or instability?  \n",
    "- Notes for integration in the final report  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a23b343",
   "metadata": {},
   "source": [
    "**Best-performing model family**\n",
    "- **Neural network regressor (MLP)** performed best overall\n",
    "- Top Model:\n",
    "  - `MLP_h20_mi500`\n",
    "  - Lowest MAE on both validation and test splits\n",
    "\n",
    "**Other strong performers**\n",
    "- KNN regressors ranked directly after MLP\n",
    "  - `KNN_k4_wdistance`\n",
    "  - `KNN_k4_wuniform`\n",
    "- Indicates:\n",
    "  - local similarity in lag space is highly informative\n",
    "  - temperature dynamics are smooth and locally consistent\n",
    "\n",
    "**Notes for final report**\n",
    "- Linear models benefitted from differencing (still performed worse due to the limited nonlinearity)\n",
    "- Classical ML models can perform **very strongly** when\n",
    "  - using proper lag engineering\n",
    "  - time leakage is strictly avoided\n",
    "- the results justify using\n",
    "  - MLP as the primary classical benchmark\n",
    "  - KNN as a robust, interpretable alternative for ML models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tis3il-ws25-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
